{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11583488,"sourceType":"datasetVersion","datasetId":7262768}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 1. Imports\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom transformers import RobertaModel, RobertaTokenizer, get_linear_schedule_with_warmup\nfrom torch.optim import AdamW\nimport re\nfrom tqdm.auto import tqdm\nimport random\nimport os\n\n# Set seeds for reproducibility\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.backends.cudnn.deterministic = True\n\nset_seed(42)\n\n# 2. Setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# 3. Load Data\ntrain_file = \"/kaggle/input/c-ours/C_Ours/data_C_Ours_train.csv\"\ntest_file = \"/kaggle/input/c-ours/C_Ours/data_C_Ours_test.csv\"\n\ntrain_df = pd.read_csv(train_file)\ntest_df = pd.read_csv(test_file)\n\n# 4. Clean Code\ndef clean_code(code):\n    code = re.sub(r'//.*?(\\n|$)', ' ', code)  # Single-line comments\n    code = re.sub(r'/\\*.*?\\*/', ' ', code, flags=re.DOTALL)  # Multi-line comments\n    code = re.sub(r'\\s+', ' ', code.strip())  # Normalize whitespace\n    return code\n\ntrain_df['code'] = train_df['code'].apply(clean_code)\ntest_df['code'] = test_df['code'].apply(clean_code)\n\n# 5. CodeBERT Tokenizer - CodeBERT is based on RoBERTa architecture\ntokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')\nmax_len = 512  # Maximum length for input\n\n# 6. Dataset with BERT tokenization \nclass CodeDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.codes = dataframe['code'].tolist()\n        self.labels = dataframe['target'].tolist()\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.codes)\n\n    def __getitem__(self, idx):\n        code = self.codes[idx]\n        label = self.labels[idx]\n        \n        # Tokenize the code with RoBERTa/CodeBERT tokenizer\n        encoding = self.tokenizer.encode_plus(\n            code,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'label': torch.tensor(label, dtype=torch.float)\n        }\n\n# 7. Create Dataset and DataLoader\ntrain_dataset = CodeDataset(train_df, tokenizer, max_len)\ntest_dataset = CodeDataset(test_df, tokenizer, max_len)\n\nbatch_size = 16  # Reduced batch size due to larger model\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# 8. LSTM+CodeBERT Model\nclass BERT_LSTM_Classifier(nn.Module):\n    def __init__(self, model_name='microsoft/codebert-base', hidden_dim=256, num_layers=2, dropout=0.3):\n        super(BERT_LSTM_Classifier, self).__init__()\n        self.bert = RobertaModel.from_pretrained(model_name)\n        self.bert_dim = self.bert.config.hidden_size\n        \n        # Bidirectional LSTM on top of BERT\n        self.lstm = nn.LSTM(\n            input_size=self.bert_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=True,\n            dropout=dropout if num_layers > 1 else 0\n        )\n        \n        # MLP head for classification\n        self.dropout = nn.Dropout(dropout)\n        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)  # * 2 for bidirectional\n        self.activation = nn.ReLU()\n        self.layernorm = nn.LayerNorm(hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, 1)\n    \n    def forward(self, input_ids, attention_mask):\n        # Get RoBERTa/CodeBERT embeddings\n        bert_output = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            return_dict=True\n        )\n        \n        # Get sequence output from BERT\n        sequence_output = bert_output.last_hidden_state  # [batch_size, seq_len, bert_dim]\n        \n        # Pass through LSTM\n        lstm_output, (hidden, _) = self.lstm(sequence_output)\n        \n        # Concatenate the final hidden states from both directions\n        # hidden shape: [num_layers * num_directions, batch_size, hidden_dim]\n        # We want the last layer's hidden state from both directions\n        hidden_fwd = hidden[-2, :, :]  # Forward direction from last layer\n        hidden_bwd = hidden[-1, :, :]  # Backward direction from last layer\n        hidden_cat = torch.cat((hidden_fwd, hidden_bwd), dim=1)  # [batch_size, hidden_dim*2]\n        \n        # MLP classifier\n        out = self.dropout(hidden_cat)\n        out = self.fc1(out)\n        out = self.activation(out)\n        out = self.layernorm(out)\n        out = self.dropout(out)\n        out = self.fc2(out)\n        \n        return out\n\n# Initialize model\nmodel = BERT_LSTM_Classifier().to(device)\n\n# 9. Loss and Optimizer\n# Use class weights to handle imbalanced data\ndef calculate_class_weights(labels):\n    class_counts = np.bincount(labels.astype(int))\n    total = len(labels)\n    weights = total / (len(class_counts) * class_counts)\n    return torch.tensor(weights, dtype=torch.float)\n\nclass_weights = calculate_class_weights(train_df['target'].values)\nprint(f\"Class weights: {class_weights}\")\n\ncriterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1].to(device))\n\n# Different learning rates for BERT and other layers\nbert_params = list(model.bert.parameters())\nother_params = list(model.lstm.parameters()) + list(model.fc1.parameters()) + list(model.fc2.parameters())\n\noptimizer = AdamW([\n    {'params': bert_params, 'lr': 2e-5},  # Lower learning rate for BERT parameters\n    {'params': other_params, 'lr': 1e-4}   # Higher learning rate for other parameters\n])\n\n# Learning rate scheduler\ntotal_steps = len(train_loader) * 10  # epochs\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0.1 * total_steps,  # 10% of total steps for warmup\n    num_training_steps=total_steps\n)\n\n# 10. Training Function\ndef train(model, loader, optimizer, criterion, scheduler):\n    model.train()\n    running_loss = 0.0\n    all_preds = []\n    all_labels = []\n    \n    for batch in tqdm(loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask).squeeze(1)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        \n        # Gradient clipping to prevent exploding gradients\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        scheduler.step()\n        \n        running_loss += loss.item()\n        \n        # Save predictions and labels for epoch-level metrics\n        preds = torch.sigmoid(outputs).detach().cpu().numpy() >= 0.5\n        all_preds.extend(preds.astype(int))\n        all_labels.extend(labels.cpu().numpy().astype(int))\n    \n    epoch_acc = accuracy_score(all_labels, all_preds)\n    epoch_report = classification_report(all_labels, all_preds, digits=4)\n    \n    return running_loss / len(loader), epoch_acc, epoch_report\n\n# 11. Evaluation Function\ndef evaluate(model, loader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    running_loss = 0.0\n    \n    with torch.no_grad():\n        for batch in tqdm(loader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n            \n            outputs = model(input_ids, attention_mask).squeeze(1)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item()\n            \n            preds = torch.sigmoid(outputs).cpu().numpy() >= 0.5\n            all_preds.extend(preds.astype(int))\n            all_labels.extend(labels.cpu().numpy().astype(int))\n    \n    acc = accuracy_score(all_labels, all_preds)\n    report = classification_report(all_labels, all_preds, digits=4)\n    cm = confusion_matrix(all_labels, all_preds)\n    \n    return running_loss / len(loader), acc, report, cm\n\n# 12. Training Loop with Early Stopping\nepochs = 7\nbest_acc = 0\npatience = 3\nno_improve_epochs = 0\n\n# Create directory for saving models\nos.makedirs('models', exist_ok=True)\n\nprint(\"Starting training...\")\nfor epoch in range(epochs):\n    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n    \n    train_loss, train_acc, train_report = train(model, train_loader, optimizer, criterion, scheduler)\n    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n    print(f\"Train Classification Report:\\n{train_report}\")\n    \n    val_loss, val_acc, val_report, val_cm = evaluate(model, test_loader)\n    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n    print(f\"Validation Classification Report:\\n{val_report}\")\n    print(f\"Confusion Matrix:\\n{val_cm}\")\n    \n    # Save the best model\n    if val_acc > best_acc:\n        best_acc = val_acc\n        torch.save(model.state_dict(), 'models/best_model.pt')\n        print(f\"New best model saved with accuracy: {best_acc:.4f}\")\n        no_improve_epochs = 0\n    else:\n        no_improve_epochs += 1\n        print(f\"No improvement for {no_improve_epochs} epochs\")\n    \n    # Early stopping\n    if no_improve_epochs >= patience:\n        print(f\"Early stopping triggered after {epoch+1} epochs\")\n        break\n\nprint(f\"Training completed. Best validation accuracy: {best_acc:.4f}\")\n\n# 13. Load the best model and evaluate on test set\nmodel.load_state_dict(torch.load('models/best_model.pt'))\ntest_loss, test_acc, test_report, test_cm = evaluate(model, test_loader)\nprint(f\"\\nFinal Test Results:\")\nprint(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\nprint(f\"Test Classification Report:\\n{test_report}\")\nprint(f\"Test Confusion Matrix:\\n{test_cm}\")\n\n# 14. Feature Importance Analysis (Optional)\ndef get_attention_visualization(model, dataset, sample_idx=0):\n    \"\"\"\n    Get attention scores for a specific code sample to see what the model focuses on\n    \"\"\"\n    model.eval()\n    sample = dataset[sample_idx]\n    \n    input_ids = sample['input_ids'].unsqueeze(0).to(device)\n    attention_mask = sample['attention_mask'].unsqueeze(0).to(device)\n    \n    with torch.no_grad():\n        outputs = model.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_attentions=True\n        )\n        \n    # Get attention weights from the last layer\n    attentions = outputs.attentions[-1].cpu().numpy()\n    \n    # Get the tokens for visualization\n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu().numpy())\n    \n    # Return tokens and attention matrix\n    return tokens, attentions\n\n# Example of how to use the feature importance analysis\nsample_idx = 0  # Choose a sample\ntokens, attentions = get_attention_visualization(model, test_dataset, sample_idx)\nprint(f\"Sample code has {len(tokens)} tokens\")\nprint(f\"First 20 tokens: {tokens[:20]}\")\nprint(f\"Attention matrix shape: {attentions.shape}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-30T04:28:49.799602Z","iopub.execute_input":"2025-04-30T04:28:49.799827Z","iopub.status.idle":"2025-04-30T06:58:55.380211Z","shell.execute_reply.started":"2025-04-30T04:28:49.799809Z","shell.execute_reply":"2025-04-30T06:58:55.379563Z"}},"outputs":[{"name":"stderr","text":"2025-04-30 04:29:15.769770: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745987356.235325      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745987356.361969      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"478f87a530044259a8bcfa0587e1b5c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"415d2b548964450c9312fb71a8960e7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"123bf08c54674a7abc060a9c91aec8f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10a1ba0b6cf7478ab64be0025ecbf020"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/498 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1091268230c4edd8653cf86f06a8cb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc1ca3b0e0a4456b96c5aff358a5b6b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1ac1d9c6048425cacb02979369c8c68"}},"metadata":{}},{"name":"stdout","text":"Class weights: tensor([1., 1.])\nStarting training...\n\nEpoch 1/7\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/677 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a4d1034fbdc461a9a197b7ef75b6b03"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.5454, Train Accuracy: 0.7194\nTrain Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.7087    0.7449    0.7264      5413\n           1     0.7312    0.6939    0.7120      5413\n\n    accuracy                         0.7194     10826\n   macro avg     0.7200    0.7194    0.7192     10826\nweighted avg     0.7200    0.7194    0.7192     10826\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/170 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ac63f6d2b5740e1aae8a24fba258f6b"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 0.4049, Validation Accuracy: 0.8356\nValidation Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.7671    0.9638    0.8542      1353\n           1     0.9513    0.7073    0.8114      1353\n\n    accuracy                         0.8356      2706\n   macro avg     0.8592    0.8356    0.8328      2706\nweighted avg     0.8592    0.8356    0.8328      2706\n\nConfusion Matrix:\n[[1304   49]\n [ 396  957]]\nNew best model saved with accuracy: 0.8356\n\nEpoch 2/7\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/677 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e2dd5d4978f40f1884d9bc2cdffcef3"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.2861, Train Accuracy: 0.8979\nTrain Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.8812    0.9198    0.9001      5413\n           1     0.9162    0.8760    0.8956      5413\n\n    accuracy                         0.8979     10826\n   macro avg     0.8987    0.8979    0.8979     10826\nweighted avg     0.8987    0.8979    0.8979     10826\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/170 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af49dbb0e8c14455b2177db98dc03e68"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 0.2695, Validation Accuracy: 0.9043\nValidation Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9404    0.8633    0.9002      1353\n           1     0.8736    0.9453    0.9081      1353\n\n    accuracy                         0.9043      2706\n   macro avg     0.9070    0.9043    0.9041      2706\nweighted avg     0.9070    0.9043    0.9041      2706\n\nConfusion Matrix:\n[[1168  185]\n [  74 1279]]\nNew best model saved with accuracy: 0.9043\n\nEpoch 3/7\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/677 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bb34716a4c04757877f8c8995033e70"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.1822, Train Accuracy: 0.9458\nTrain Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9332    0.9603    0.9466      5413\n           1     0.9591    0.9313    0.9450      5413\n\n    accuracy                         0.9458     10826\n   macro avg     0.9462    0.9458    0.9458     10826\nweighted avg     0.9462    0.9458    0.9458     10826\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/170 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"729f2e076fb4452bb913944afc7767f5"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 0.2356, Validation Accuracy: 0.9276\nValidation Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.8849    0.9830    0.9314      1353\n           1     0.9809    0.8721    0.9233      1353\n\n    accuracy                         0.9276      2706\n   macro avg     0.9329    0.9276    0.9273      2706\nweighted avg     0.9329    0.9276    0.9273      2706\n\nConfusion Matrix:\n[[1330   23]\n [ 173 1180]]\nNew best model saved with accuracy: 0.9276\n\nEpoch 4/7\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/677 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5382663178614a61b69b01720dbd659a"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.1192, Train Accuracy: 0.9678\nTrain Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9575    0.9789    0.9681      5413\n           1     0.9785    0.9566    0.9674      5413\n\n    accuracy                         0.9678     10826\n   macro avg     0.9680    0.9678    0.9678     10826\nweighted avg     0.9680    0.9678    0.9678     10826\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/170 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"105f9046661b487793b0d2b991b2e3f9"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 0.2325, Validation Accuracy: 0.9464\nValidation Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9370    0.9571    0.9470      1353\n           1     0.9562    0.9357    0.9458      1353\n\n    accuracy                         0.9464      2706\n   macro avg     0.9466    0.9464    0.9464      2706\nweighted avg     0.9466    0.9464    0.9464      2706\n\nConfusion Matrix:\n[[1295   58]\n [  87 1266]]\nNew best model saved with accuracy: 0.9464\n\nEpoch 5/7\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/677 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27d3ade05e48439fb1013876137c059d"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.0796, Train Accuracy: 0.9800\nTrain Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9743    0.9861    0.9802      5413\n           1     0.9860    0.9740    0.9799      5413\n\n    accuracy                         0.9800     10826\n   macro avg     0.9801    0.9800    0.9800     10826\nweighted avg     0.9801    0.9800    0.9800     10826\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/170 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcfc549693d141e8b610fc096f99696a"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 0.2188, Validation Accuracy: 0.9446\nValidation Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9337    0.9571    0.9453      1353\n           1     0.9560    0.9320    0.9439      1353\n\n    accuracy                         0.9446      2706\n   macro avg     0.9448    0.9446    0.9446      2706\nweighted avg     0.9448    0.9446    0.9446      2706\n\nConfusion Matrix:\n[[1295   58]\n [  92 1261]]\nNo improvement for 1 epochs\n\nEpoch 6/7\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/677 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a33c466c1a3742a9ab41499a13aae42d"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.0591, Train Accuracy: 0.9859\nTrain Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9840    0.9878    0.9859      5413\n           1     0.9878    0.9839    0.9858      5413\n\n    accuracy                         0.9859     10826\n   macro avg     0.9859    0.9859    0.9859     10826\nweighted avg     0.9859    0.9859    0.9859     10826\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/170 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d590bd52785e4eb1aff2dae24fa124bd"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 0.2432, Validation Accuracy: 0.9501\nValidation Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9381    0.9638    0.9508      1353\n           1     0.9628    0.9364    0.9494      1353\n\n    accuracy                         0.9501      2706\n   macro avg     0.9504    0.9501    0.9501      2706\nweighted avg     0.9504    0.9501    0.9501      2706\n\nConfusion Matrix:\n[[1304   49]\n [  86 1267]]\nNew best model saved with accuracy: 0.9501\n\nEpoch 7/7\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/677 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e981dcd9a957423f9cacbefb198bb808"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.0364, Train Accuracy: 0.9913\nTrain Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9902    0.9924    0.9913      5413\n           1     0.9924    0.9902    0.9913      5413\n\n    accuracy                         0.9913     10826\n   macro avg     0.9913    0.9913    0.9913     10826\nweighted avg     0.9913    0.9913    0.9913     10826\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/170 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e96cc37b5944f85ad0eea8865f8d8e8"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 0.3253, Validation Accuracy: 0.9464\nValidation Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9597    0.9320    0.9456      1353\n           1     0.9339    0.9608    0.9472      1353\n\n    accuracy                         0.9464      2706\n   macro avg     0.9468    0.9464    0.9464      2706\nweighted avg     0.9468    0.9464    0.9464      2706\n\nConfusion Matrix:\n[[1261   92]\n [  53 1300]]\nNo improvement for 1 epochs\nTraining completed. Best validation accuracy: 0.9501\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3499950466.py:283: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('models/best_model.pt'))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/170 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"376e5e55479b44918e152b772f2e3884"}},"metadata":{}},{"name":"stderr","text":"RobertaSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n","output_type":"stream"},{"name":"stdout","text":"\nFinal Test Results:\nTest Loss: 0.2432, Test Accuracy: 0.9501\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9381    0.9638    0.9508      1353\n           1     0.9628    0.9364    0.9494      1353\n\n    accuracy                         0.9501      2706\n   macro avg     0.9504    0.9501    0.9501      2706\nweighted avg     0.9504    0.9501    0.9501      2706\n\nTest Confusion Matrix:\n[[1304   49]\n [  86 1267]]\nSample code has 512 tokens\nFirst 20 tokens: ['<s>', 'SY', 'SC', 'ALL', '_', 'DE', 'FINE', '3', '(', 'os', 'f', '_', 'sys', 'info', ',', 'Ġint', ',', 'Ġcommand', ',', 'Ġchar']\nAttention matrix shape: (1, 12, 512, 512)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}